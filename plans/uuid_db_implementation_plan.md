# UUID + Database Implementation Plan

## ðŸ“Š Implementation Progress
**Overall Completion: ~60% (6/10 phases completed)**

### âœ… **Completed Phases (6/10):**
- Phase 1: Dependencies & Configuration
- Phase 2: Data Structure Updates  
- Phase 3: Input Parsing Updates
- Phase 8: Testing & Validation (Partial - UUID/CSV parts)

### ðŸ”„ **In Progress:**
- All tests now passing (88/88) with UUID-based data structures
- CSV parsing fully supports new `artist_id,artist_name,artist_data` format
- Core infrastructure ready for database integration

### ðŸš§ **Remaining Phases (4/10):**
- Phase 4: Database Integration (Database connection/operations)
- Phase 5: CLI Argument Updates (Database-related flags)
- Phase 6: Processing Logic Updates (Database write integration)
- Phase 7: Output Format Updates (Database status reporting)

## Overview
Transform `run_artists.py` to work with UUID-based artist IDs and persist generated bios to PostgreSQL database.

## Current State Analysis
- âœ… Existing: CSV parsing, OpenAI API calls, concurrent processing, JSONL output
- âœ… **NEW**: UUID support, CSV format validation, comprehensive test coverage
- ðŸ”„ Still missing: Database connectivity, bio persistence
- ðŸ”„ Needs completion: Database integration, CLI arguments, write modes

## Implementation Tasks

### Phase 1: Dependencies & Configuration
**Priority: High | Estimated Time: 1-2 hours** âœ… **COMPLETED**

#### Task 1.1: Add Database Dependencies
- âœ… Add `psycopg3[binary]` to `requirements.txt`
- âœ… Update `.env.example` with `DATABASE_URL` template
- âœ… Document database connection requirements in README

#### Task 1.2: Environment Variable Support
- âœ… Add `DATABASE_URL` environment variable handling
- âœ… Update environment variable validation logic
- âœ… Note: Model selection will use server defaults, no client-side model selection needed

### Phase 2: Data Structure Updates
**Priority: High | Estimated Time: 2-3 hours** âœ… **COMPLETED**

#### Task 2.1: Update ArtistData Class
- âœ… **COMPLETED** - Updated to include `artist_id` UUID field
```python
class ArtistData(NamedTuple):
    artist_id: str  # UUID string
    name: str
    data: Optional[str] = None
```

#### Task 2.2: Update ApiResponse Class
- âœ… **COMPLETED** - Added `artist_id` and `db_status` fields
```python
class ApiResponse(NamedTuple):
    artist_id: str  # Add UUID
    artist_name: str
    artist_data: Optional[str]
    response_text: str
    response_id: str
    created: int
    db_status: Optional[str] = None  # "updated|skipped|error|null"
    error: Optional[str] = None
```

#### Task 2.3: Create Database Models
- ðŸ”„ **PENDING** - Database classes not implemented yet
```python
class DatabaseConfig(NamedTuple):
    url: str
    pool_size: int = 5
    max_overflow: int = 10

class DatabaseResult(NamedTuple):
    success: bool
    rows_affected: int
    error: Optional[str] = None
```

### Phase 3: Input Parsing Updates
**Priority: High | Estimated Time: 2-3 hours** âœ… **COMPLETED**

#### Task 3.1: Update CSV Parser
- âœ… Modify `parse_input_file()` to handle `artist_id,artist_name,artist_data` format
- âœ… Add UUID validation for `artist_id` field
- âœ… Add proper CSV parsing with `csv` module (quote-aware)
- âœ… Handle optional header row
- âœ… Note: No backward compatibility - only support new UUID format

#### Task 3.2: Input Validation
- âœ… Validate UUID format for `artist_id`
- âœ… Ensure `artist_name` is non-empty
- âœ… Handle empty `artist_data` gracefully
- âœ… Add comprehensive error reporting for invalid input

#### Task 3.3: Update Example Files
- âœ… Replace `example_artists.csv` with new UUID format
- âœ… Add sample data with various UUID formats
- âœ… Create test data for validation scenarios

### Phase 4: Database Integration
**Priority: High | Estimated Time: 4-5 hours** ðŸš§ **PENDING**

#### Task 4.1: Database Connection Management
```python
def create_db_connection_pool(config: DatabaseConfig) -> psycopg3.Pool
def get_db_connection(pool: psycopg3.Pool) -> psycopg3.Connection
def close_db_connection_pool(pool: psycopg3.Pool) -> None

# Sensible defaults for connection pooling
DEFAULT_POOL_SIZE = 4  # Match default worker count
DEFAULT_MAX_OVERFLOW = 8  # Allow burst connections
DEFAULT_CONNECTION_TIMEOUT = 30  # seconds
DEFAULT_QUERY_TIMEOUT = 60  # seconds
```

#### Task 4.2: Database Write Operations
```python
@retry_with_exponential_backoff(max_retries=3)
def update_artist_bio(
    connection: psycopg3.Connection,
    artist_id: str,
    bio: str,
    skip_existing: bool = False,
    worker_id: str = "main"
) -> DatabaseResult

# Error handling strategy:
# - Permanent errors (invalid UUID, constraint violations): Skip and log
# - Transient errors (connection timeout, deadlock): Retry with backoff
# - Systemic errors (auth failure, schema issues): Abort processing
```

#### Task 4.3: Write Mode Logic
- [ ] Implement `--write-mode {db,file,both}` logic
- [ ] Add `--skip-existing` flag behavior
- [ ] Handle database connection failures gracefully
- [ ] Add database transaction management

#### Task 4.4: SQL Query Implementation
```sql
-- Default (force overwrite)
UPDATE artists SET bio = $2 WHERE id = $1;

-- Skip existing
UPDATE artists SET bio = $2 WHERE id = $1 AND bio IS NULL;
```

### Phase 5: CLI Argument Updates
**Priority: Medium | Estimated Time: 1-2 hours** ðŸš§ **PENDING**

#### Task 5.1: Add New Arguments
- [ ] `--db-url STRING` (default: `DATABASE_URL` env var)
- [ ] `--write-mode {db,file,both}` (default: `db`)
- [ ] `--skip-existing` (flag)
- [ ] Note: Model selection removed - will use server defaults

#### Task 5.2: Update Existing Arguments
- [ ] Modify `--input-file` help text for new format
- [ ] Update `--output` behavior for different write modes
- [ ] Enhance `--dry-run` to show database operations

#### Task 5.3: Argument Validation
- [ ] Validate `--write-mode` choices
- [ ] Validate `--db-url` format
- [ ] Add mutual exclusivity checks where needed
- [ ] Remove model validation (using server defaults)

### Phase 6: Processing Logic Updates
**Priority: High | Estimated Time: 3-4 hours** ðŸš§ **PENDING**

#### Task 6.1: Update Concurrent Processing
- [ ] Modify `process_artists_concurrent()` to handle database writes
- [ ] Add database connection pool management
- [ ] Implement write mode logic in processing loop
- [ ] Add database error handling and retries

#### Task 6.2: Update API Response Handling
- [ ] Modify `call_openai_api()` to return artist_id
- [ ] Add database write status to response tracking
- [ ] Update progress logging to include database operations

#### Task 6.3: Enhanced Error Handling
- [ ] Add database-specific error types
- [ ] Implement database retry logic
- [ ] Add connection pool error recovery
- [ ] Update error reporting in JSONL output

### Phase 7: Output Format Updates
**Priority: Medium | Estimated Time: 2-3 hours** ðŸš§ **PENDING**

#### Task 7.1: Update JSONL Output Schema
```json
{
  "artist_id": "uuid",
  "artist_name": "...",
  "artist_data": "...",
  "response_text": "...",
  "response_id": "...",
  "db_status": "updated|skipped|error|null",
  "error": null
}
```
Note: Removed "model" field since using server defaults

#### Task 7.2: Write Mode Output Logic
- [ ] Handle `--write-mode file` (existing JSONL behavior)
- [ ] Handle `--write-mode db` (database only, minimal JSONL)
- [ ] Handle `--write-mode both` (database + full JSONL simultaneously)

#### Task 7.3: Dry Run Enhancements
- [ ] Show database update previews
- [ ] Display connection pool configuration
- [ ] Preview SQL queries for first 5 artists

### Phase 8: Testing & Validation
**Priority: High | Estimated Time: 4-6 hours** âœ… **PARTIALLY COMPLETED**

#### Task 8.1: Unit Tests
- âœ… Test UUID validation logic
- âœ… Test new CSV parsing format
- ðŸ”„ Test database connection management (pending database implementation)
- ðŸ”„ Test write mode logic (pending database implementation)
- âœ… Test error handling scenarios

#### Task 8.2: Integration Tests
- ðŸ”„ Create test database schema (`test_artists` table) (pending)
- ðŸ”„ Test with real database using test schema (pending)
- ðŸ”„ Test concurrent database operations (pending)
- ðŸ”„ Test retry logic for database failures (pending)
- ðŸ”„ Test different write modes (pending)
- ðŸ”„ Test error handling (permanent, transient, systemic) (pending)

#### Task 8.3: End-to-End Tests
- âœ… Test complete workflow with sample data (file mode)
- ðŸ”„ Test performance with large datasets (1000+ artists) (pending)
- ðŸ”„ Test error recovery scenarios (pending)
- ðŸ”„ Test database connection failure recovery (pending)

### Phase 9: Documentation & Examples
**Priority: Medium | Estimated Time: 2-3 hours**

#### Task 9.1: Update Documentation
- [ ] Update README with new usage examples
- [ ] Document database setup requirements
- [ ] Add troubleshooting guide for database issues
- [ ] Update CLI help text and examples

#### Task 9.2: Create Examples
- [ ] Create sample UUID CSV files
- [ ] Add database setup scripts
- [ ] Add performance tuning examples

### Phase 10: Deployment Preparation
**Priority: Medium | Estimated Time: 1 hour**

#### Task 10.1: Deployment Preparation
- [ ] Update deployment scripts
- [ ] Add database schema setup scripts
- [ ] Update environment configuration

## Technical Considerations

### Database Connection Management
- Use connection pooling for concurrent operations
- Implement proper connection lifecycle management
- Add connection health checks
- Handle connection timeouts gracefully

### Error Handling Strategy
- Database errors should not stop processing other artists
- Implement exponential backoff for database retries
- Log database errors separately from API errors
- Provide clear error messages for troubleshooting

### Performance Considerations
- Database writes should be batched where possible
- Connection pool size should match worker count
- Add database query performance monitoring
- Consider read replicas for large datasets

### Security Considerations
- Validate all UUID inputs to prevent injection
- Use parameterized queries exclusively
- Secure database connection strings
- Add audit logging for database operations

## Risk Mitigation

### High Risk Items
1. **Database Connection Failures**: Implement robust retry logic and fallback modes
2. **UUID Validation**: Add comprehensive validation to prevent malformed data
3. **Concurrent Database Access**: Use proper transaction management and locking

### Medium Risk Items
1. **Performance Degradation**: Monitor database performance and optimize queries
2. **Data Consistency**: Implement proper error handling for partial failures
3. **Configuration Complexity**: Provide clear documentation and examples

## Success Criteria

### Functional Requirements
- âœ… Successfully parse UUID-based CSV input
- âœ… Generate artist bios using OpenAI API
- ðŸ”„ Persist bios to PostgreSQL database (pending implementation)
- ðŸ”„ Support all write modes (db, file, both) (pending implementation)
- ðŸ”„ Handle database errors gracefully (pending implementation)
- âœ… Maintain concurrent processing performance

### Non-Functional Requirements
- ðŸ”„ Process 1000+ artists without performance degradation (pending full testing)
- ðŸ”„ Handle database connection failures without data loss (pending implementation)
- âœ… Provide comprehensive error reporting
- âœ… Support dry-run mode for testing

### Quality Requirements
- âœ… 95%+ test coverage for new functionality (UUID parsing & validation complete)
- âœ… All existing tests continue to pass (88/88 tests passing)
- âœ… Performance within 5% of current implementation
- âœ… Clear documentation and examples
- âœ… Proper error handling and logging

## Timeline Estimate

**Total Estimated Time: 24-34 hours**

- **Phase 1-2 (Dependencies & Data Structures)**: 3-5 hours
- **Phase 3 (Input Parsing)**: 2-3 hours  
- **Phase 4 (Database Integration)**: 4-5 hours
- **Phase 5 (CLI Updates)**: 1-2 hours
- **Phase 6 (Processing Logic)**: 3-4 hours
- **Phase 7 (Output Format)**: 2-3 hours
- **Phase 8 (Testing)**: 4-6 hours
- **Phase 9 (Documentation)**: 2-3 hours
- **Phase 10 (Deployment)**: 1 hour

## Dependencies

### External Dependencies
- `psycopg3[binary]` - PostgreSQL adapter
- Existing OpenAI client
- Existing concurrent processing framework

### Internal Dependencies
- Current CSV parsing logic (to be modified)
- Current API calling logic (to be enhanced)
- Current error handling framework (to be extended)
- Current logging system (to be enhanced)

## Implementation Decisions Made

1. **Database Schema**: Using simplified schema from prompt (`id UUID, name TEXT, bio TEXT`)
2. **Model Selection**: Removed - will use server defaults (no client-side model selection)
3. **Write Mode Behavior**: `--write-mode both` writes to both database AND file simultaneously
4. **Database Library**: Using psycopg3 for modern async support and better performance
5. **OpenAI API**: Keeping existing API call structure unchanged

## Implementation Decisions Made (Updated)

1. **Database Schema**: Using simplified schema from prompt (`id UUID, name TEXT, bio TEXT`)
2. **Model Selection**: Removed - will use server defaults (no client-side model selection)
3. **Write Mode Behavior**: `--write-mode both` writes to both database AND file simultaneously
4. **Database Library**: Using psycopg3 for modern async support and better performance
5. **OpenAI API**: Keeping existing API call structure unchanged
6. **Backward Compatibility**: None - will only support new UUID-based CSV format
7. **Database Connection**: Use sensible defaults for connection pooling
8. **Error Handling**: Skip/log permanent errors, retry transient errors, abort on systemic errors
9. **Performance Requirements**: Target 1000+ artists/hour with <5% overhead vs current implementation
10. **Testing Database**: Create dedicated test schema for isolated testing

## Performance Recommendations

### Target Performance Metrics:
- **Throughput**: 1000+ artists processed per hour
- **Overhead**: <5% performance degradation vs current file-only implementation
- **Database Operations**: <100ms average per artist bio update
- **Concurrency**: Maintain current 4-worker default with database connection pooling
- **Memory Usage**: <50MB additional memory for database connections

### Optimization Strategies:
- Connection pooling with 4-8 connections (matching worker count)
- Batch database operations where possible
- Use prepared statements for repeated UPDATE queries
- Implement connection health checks and automatic reconnection
- Add database query performance monitoring